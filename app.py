import requests
import json
import os  # í´ë” ìƒì„±ìš©
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFacePipeline
import torch
_ = torch.classes

# API Key ì…ë ¥
api_key = "up_P5UsiWxIdS9sAyUCQmc9R5krA9dQr"
url = "https://api.upstage.ai/v1/document-ai/document-parse"

headers = {
    "Authorization": f"Bearer {api_key}"
}

pdf_files = ["ì‚¼ì„±.pdf", "í•˜ë‚˜.pdf", "í† ìŠ¤.pdf", "ìš°ë¦¬.pdf"]

# ì €ì¥í•  í´ë” ê²½ë¡œ (card-rag-app/parsed_html)
save_dir = "parsed_html"
os.makedirs(save_dir, exist_ok=True)  # í´ë” ì—†ìœ¼ë©´ ìë™ ìƒì„±

# ì—¬ëŸ¬ ê°œì˜ íŒŒì¼ì„ ë³€í™˜í•˜ëŠ” ë°˜ë³µë¬¸
for filename in pdf_files:
    with open(filename, "rb") as file:
        files = {"document": file}

        data = {
            "ocr": "force",
            "coordinates": True,
            "chart_recognition": True,
            "output_formats": ["html"],
            "base64_encoding": [],
            "model": "document-parse"
        }

        response = requests.post(url, headers=headers, files=files, json=data)
        result = response.json()

        html_text = result['content']['html']
        card_name = filename.split(".")[0]
        output_path = os.path.join(save_dir, f"{card_name}.html")
        print(f"Saving {output_path}")

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_text)

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# HTML íŒŒì¼ì´ ì €ì¥ëœ í´ë”
html_dir = "parsed_html"
chunks = []

# HTML íŒŒì¼ ì½ê³  í…ìŠ¤íŠ¸ ë¶„í• 
for filename in os.listdir(html_dir):
    if filename.endswith(".html"):
        path = os.path.join(html_dir, filename)
        with open(path, "r", encoding="utf-8") as f:
            text = f.read()

        splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50,
                                                  separators=["\n\n", "\n", ".", " ", ""])
        card_chunks = splitter.split_text(text)
        print(f"âœ… {filename}: {len(card_chunks)}ê°œ chunk ìƒì„±ë¨")
        chunks.extend(card_chunks)

chunks = chunks[:20]

if not chunks:
    raise ValueError("â— HTMLì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

vector_db = FAISS.from_texts(chunks, embedding_model)
vector_db.save_local("vector_db")
print("âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ! â†’ vector_db/")

# ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db = FAISS.load_local("vector_db", embedding_model, allow_dangerous_deserialization=True)

# HuggingFace LLM íŒŒì´í”„ë¼ì¸ (flan-t5-base)
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

qa_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=512)
llm = HuggingFacePipeline(pipeline=qa_pipeline)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_db.as_retriever(),
    return_source_documents=True
)

query = "ë°°ë‹¬ì•± í• ì¸ë˜ëŠ” ì¹´ë“œ ë­ ìˆì–´?"
result = qa.invoke(query)

print("ğŸ’¬ ì‘ë‹µ:\n", result["result"])
print("\nğŸ“„ ì°¸ì¡°ëœ ì¹´ë“œ í˜œíƒ ë¬¸ì„œ:\n")
for doc in result["source_documents"]:
    print("â€”â€”â€”")
    print(doc.page_content)

# --- Streamlit ì•± ì‹œì‘ ---
st.set_page_config(page_title="ì¹´ë“œ ì¶”ì²œ ì±—ë´‡")
st.title("ğŸ’³ ëŒ€í•™ìƒ ëŒ€ìƒ ì¹´ë“œ ì¶”ì²œ AI ì±—ë´‡")
st.markdown("ì¹´ë“œ ì•½ê´€ ê¸°ë°˜ìœ¼ë¡œ í˜œíƒì„ ë¶„ì„í•´ì£¼ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.")

user_query = st.text_input("â“ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ë°°ë‹¬ì•± í• ì¸ ì¹´ë“œ ë­ ìˆì–´?")

if user_query:
    with st.spinner("ğŸ¤– AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):

        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vector_db = FAISS.load_local("vector_db", embedding, allow_dangerous_deserialization=True)

        model_name = "google/flan-t5-small"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            truncation=True
        )

        retriever = vector_db.as_retriever(search_kwargs={"k": 1})
        docs = retriever.get_relevant_documents(user_query)

        context_raw = docs[0].page_content
        question_raw = user_query

        question_tokens = tokenizer.encode(f"ì§ˆë¬¸:\n{question_raw}", add_special_tokens=False)
        max_input_tokens = 512
        buffer_tokens = 10
        max_context_tokens = max_input_tokens - len(question_tokens) - buffer_tokens

        context_tokens = tokenizer.encode(context_raw, add_special_tokens=False)[:max_context_tokens]
        context_text = tokenizer.decode(context_tokens, skip_special_tokens=True)

        final_prompt = f"ì¹´ë“œ í˜œíƒ ì„¤ëª…:\n{context_text}\n\nì§ˆë¬¸:\n{question_raw}"
        input_ids = tokenizer.encode(final_prompt, return_tensors="pt")

        output = model.generate(input_ids, max_new_tokens=128)
        final_answer = tokenizer.decode(output[0], skip_special_tokens=True)

    st.markdown("### ğŸ’¬ ì‘ë‹µ")
    st.write(final_answer or "âš ï¸ ë‹µë³€ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì…ë ¥ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    with st.expander("ğŸ“„ ì°¸ì¡°ëœ ë¬¸ì„œ"):
        for doc in docs:
            st.markdown("â€”â€”â€”")
            st.write(doc.page_content)
